---
title: 'Event-related potentials: why and how I used them'
author: ''
date: '2017-01-01'
slug: event-related-potentials-why-and-how-I-used-them
categories:
  - event-related potentials
tags:
  - event-related potentials
  - electroencephalography
  - methodology
  - cognitive neuroscience
  - psycholinguistics
subtitle: ''
summary: ''
authors: []
lastmod: ''
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---


Event-related potentials, or ERPs, offer a unique insight in the study of how the brain helps us think. Let's look at their reason-to-be for the purposes of research, and how they are defined and processed (this order follows the final relevance of things, although it is chronologically inverse). Most of this content is based on my [master's thesis](https://psyarxiv.com/5gjvk)).

## What are ERPs and how are they studied

ERPs are electrical responses in the brain that are elicited by a specific stimulus. They are constantly produced, for every one of our actions. These overlapping electrical pulses happen at extremely high frequencies. Indeed, the signal can be measured once per millisecond. Yeah—picture the electrical :fireworks: in your brain. The high frequency of this signal is very interesting for the study of some cognitive processes in particular, for which the time course is (or may be) critical. One such example is conceptual processing—that is, how people understand the meaning of words.

## Time course of word processing

Processing a word takes slightly less than a second—around 800 milliseconds (ms). Within this period, earlier processes (compared to later ones) have been ascribed greater relevance to the core process of understanding a word (Mahon & Caramazza, 2008). This assumes that broader processes start only after more immediate ones have started (but see Lebois, Wilson‐Mendenhall & Barsalou, 2014). The most immediate process is the recognition of a string of letters, which seems to start within 90 ms post word onset in early auditory cortex and the Visual Word Form Area (Willems, Frank, Nijhoff, Hagoort, & van den Bosch, 2016). Then ensue further, fundamental stages known as *lexical* and *semantic* processes. Lexical processing is the identification of a string of letters as a known word, and it happens within around 160 ms post word onset. Next, at around 200 ms, we may see the beginning of semantic processing, which denotes a further step in the cognitive analysis of the word that is akin to *meaning* (Hauk, 2016). These processes may overlap, as indeed suggested by the sensitivity of the N400 ERP (see also next section) to both lexical and semantic tasks (Kutas & Federmeier, 2011). Both processes also likely extend further in the processing timeline (Hauk, 2016). Nonetheless, lexical processing is still generally accepted as a precursor of sorts to semantic processing. One of the reasons why this is important to psycholinguists is that lexical and semantic processing have been differentially linked to certain cognitive phenomena. For instance, tasks promoting semantic processing (e.g., semantic decision, whereby participants describe words as concrete or abstract) have been found to engage sensorimotor simulation of the word's meaning (known as *embodiment*) more strongly than lexical tasks do (Connell & Lynott, 2013; Sato, Mengarelli, Riggio, Gallese, & Buccino, 2008).

Once the lexical and semantic stages have emerged, post-lexical, post-semantic processes follow (Mahon & Caramazza, 2008). These are mental imagery and episodic memory processes—both with an approximate emergence around 270 ms after word onset. The gradual progression from the identification of a word up to accessing its broadest meaning is an important anchoring point in the current research on the alleged embodiment of meaning comprehension, even if we might hope to count on more definitive threshold points (Hauk, 2016). 

Word processing data are mainly based on written word processing, but spoken words are processed quite similarly, if slightly faster (Leonard, Baud, Sjerps, & Chang, 2016; Pulvermüller, Shtyrov, & Ilmoniemi, 2005; Shtyrov, Hauk, & Pulvermüller, 2004). 

The bigger take-home messages would be: (1) the processing of meaning might only start at around 160 ms post word onset, and (2) processes outside of meaning comprehension might only start at around 270 ms. These working references must be taken with some caution because particular semantic effects have been found at different stages (e.g., the conceptual modality switch, as in Hald, Marshall, Janssen, & Garnham, 2011; Collins, Pecher, Zeelenberg, & Coulson, 2011). Indeed, in an influential critique of blooming findings on embodiment, Mahon and Caramazza (2008) argued that even early effects might possibly be explained in terms of non-embodied processing. They contended that working memory processes that were ancillary rather than semantic could be quickly engaged with the function of ‘colouring’ a concept, not building it up. To further complicate the matter, we do not have absolute certainty on the later section of the time course. Thus, as Hauk (2016) reviews, the different stages likely overlap at certain points, with different degrees of relevance. For instance, lexical processing may continue even once semantic processing has started, but would naturally become less relevant. Indeed, the relation among these processes is likely more of a continuum than a set of clear-cut modules. In a nutshell, the time course is important with some experimental effects in word processing, and, to that extent, we depend on our knowledge of the basic time course of word processing.

The conceptual modality switch paradigm and its time course

In demonstrating the relevance of embodied cognition, a sizeable series of studies have shown that reading about different conceptual modalities (e.g., auditory ‘loud bell’ followed by visual ‘pink background’) incurs processing costs (Pecher, Zeelenberg, & Barsalou, 2003). Importantly, this manipulation does not concern the presentation mode of the stimulus, maintained constant, but the intrinsic semantic modality of the stimulus concepts. The conceptual modality switch effect has often been replicated (Pecher, Zeelenberg, & Barsalou, 2004; Solomon & Barsalou, 2004; Marques, 2006; Vermeulen, Niedenthal, & Luminet, 2007; van Dantzig, Pecher, Zeelenberg, & Barsalou, 2008; Lynott & Connell, 2009; Ambrosi, Kalenine, Blaye, & Bonthoux, 2011; Collins et al., 2011; Hald et al., 2011; Hald et al., 2013; Scerrati et al., 2015).

Bernabeu, Willems and Louwerse (2017) addressed a caveat with the time course of the conceptual modality switch paradigm. In previous experiments, trials presented a concept word followed by a property word. ERPs were time-locked to the latter property word. This design may have left uncontrolled a switch produced already at the concept. Indeed, the property word was already supposed to be in the particular modality of the trial. That pitfall could have had two consequences: loss of power and loss of certainty on the time course of the effect. Thus, Bernabeu et al. created a design in which ERPs were time-locked to the very first word in each trial (see some [early input from researchers online](https://www.researchgate.net/post/Conceptual_modality_switch_effect_measured_at_first_word)). The purpose of this relocation was not to completely annul the possibility of post-core sensory processes, but to at least measure the modality switch doubtlessly from its start.

## ERP components

Researchers are typically interested in specific *components* of the ERP signal. When plotted, components have the shape of waves—a peak with descending tails on each side. Thus we talk of ERP *waveforms*. Multiple components are known, each having been found to consistently peak around specific points in time during a cognitive process. The peak is one of the features we use to describe each component (van Hell & Kroll, 2013).

**1. Polarity:** the component either peaks in the positive or the negative pole of the signal. This polarity is relative to the *baseline* point that is created in the pre-processing stage (see below);

**2. latency:** the time course of the component, encompassing an onset, a peak and an overall duration;

**3. amplitude:** the voltage reached at a given time or for a given period;

**4. topographic scalp distribution:** the areas in the scalp (the scalp being a reasonable proxy for the brain) in which the component appears;

**5. functional role:** the cognitive functions that have been consistently associated with the component.

Examples of components in language processing include the N400, consistently linked to semantic processing, that is, seeking the meaning of words or sentences. The N400 is characterised by a large, negative amplitude peaking at around 400 ms post word onset, primarily found in central and posterior sites. N400 *effects*, which are comparisons of the N400 component in different experimental conditions, have consistently appeared under violations of semantic expectations, i.e., related to meaning and events (Kutas & Federmeier, 2011; Swaab, Ledoux, Camblin, & Boudewyn, 2011). Another well-known component in language is the P600, linked to syntactic processing, that is, the structure of sentences (Swaab et al., 2011). Other examples of components include lateralized readiness potentials, signalling motor preparation (Mordkoff & Gianaros, 2000), and the P3b component, which appears in the context of responses (van Vliet et al., 2014). Both the latter are relevant to researchers across domains, who often ward off against the influence of these components in their experiments.






```
  <Nodes>
    <string>1/Raw Data</string>
    <string>1/Raw Data/labels</string>
    <string>1/Raw Data/labels/positions</string>
    <string>1/Raw Data/labels/positions/rerefRM</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMatch</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMatch</string>
    <string>1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch</string>
    <string>1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection</string>
    <string>1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr</string>
    <string>1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif</string>
    <string>1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_EmbodiedMismatch</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMismatch</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif</string>
    <string>1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMismatch</string>
  </Nodes>
```


References


Van Hell, J. G., & Kroll, J. F. (2013). Using electrophysiological measures to track the mapping of words to concepts in the bilingual brain: a focus on translation. In J. Altarriba & L. Isurin (Eds.), *Memory, Language, and Bilingualism: Theoretical and Applied Approaches* (pp. 126-160). New York: Cambridge University Press.
