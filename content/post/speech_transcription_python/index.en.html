---
title: 'Secure and scalable speech transcription for local and HPC'
author: ''
date: '2025-11-12'
slug: speech-transcription-python
categories:
  - programming
tags:
  - s
  - speech-to-text
  - speech recognition
  - transcription
  - Whisper
  - machine learning
  - huggingface
  - Python
  - OpenAI
  - natural language processing
  - audio processing
  - GDPR
  - data protection
  - data privacy
  - high-performance computing
  - privacy
subtitle: ''
summary: "A production-ready local transcription workflow leveraging OpenAI's Whisper models that addresses the limitations of cloud-based solutions through complete data sovereignty, unlimited scale, reproducible processing and advanced quality control, while maintaining GDPR compliance."
authors: []
lastmod: ~
featured: no
draft: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<div id="introduction-the-evolution-of-transcription-technology" class="section level2">
<h2>Introduction: The Evolution of Transcription Technology</h2>
<p>The landscape of speech-to-text transcription has undergone a remarkable transformation in recent years, driven by the proliferation of generative artificial intelligence (genAI) tools. From basic dictation software to sophisticated neural networks, transcription technology has evolved to handle diverse audio conditions, multiple languages and complex speech patterns with unprecedented accuracy.</p>
<p>At the forefront of this revolution stands <a href="https://github.com/openai/whisper">OpenAI’s Whisper model family</a>, released as open-source tools that have democratised access to state-of-the-art <a href="https://huggingface.co/models?pipeline_tag=automatic-speech-recognition">automatic speech recognition (ASR)</a> capabilities. True to the “Open” in OpenAI’s original mission, these models have become the gold standard for transcription tasks, offering researchers and developers robust, multilingual speech recognition that rivals proprietary commercial solutions. The Whisper architecture, trained on 680,000 hours of multilingual audio data, represents a paradigm shift toward generalisable, production-ready ASR systems that can handle real-world audio conditions without extensive fine-tuning.</p>
</div>
<div id="chatbots-and-limitations" class="section level2">
<h2>Chatbots and Limitations</h2>
<p>Large Language Model chatbots such as <a href="https://chat.openai.com/">ChatGPT</a> and <a href="https://gemini.google.com/">Google Gemini</a> allow uploading recordings and having them transcribed using advanced models such as Whisper. However, this route has several limitations that make it unsuitable for serious research and production workflows.</p>
<p>First, cloud-based chatbot interfaces impose strict file size limitations, typically restricting uploads to recordings of 25MB or less, which translates to roughly 20-30 minutes of audio content. This constraint renders them impractical for transcribing lengthy interviews, focus groups, or extended research sessions that often span multiple hours.</p>
<p>Second, chatbot-based transcription provides significantly less reproducibility than local workflows. The exact model versions, processing parameters and post-processing steps remain opaque to users, making it impossible to replicate results or maintain consistent transcription quality across different sessions. This lack of transparency is particularly problematic for academic research where methodological rigor and reproducibility are paramount.</p>
<p>Third, when working with confidential or sensitive recordings, cloud-based solutions introduce substantial privacy and compliance risks. While some platforms offer temporary chat modes that allegedly prevent model training using uploaded content, this approach still requires transmitting sensitive audio data to third-party servers, potentially violating institutional policies, research ethics requirements, or data protection regulations such as the <a href="https://gdpr.eu/">General Data Protection Regulation (GDPR)</a>.</p>
</div>
<div id="addressing-the-limitations-python-supported-local-workflow" class="section level2">
<h2>Addressing the Limitations: Python-Supported Local Workflow</h2>
<p>The constraints of cloud-based transcription can be comprehensively addressed by implementing a secure, self-contained, local workflow that leverages the same advanced models while maintaining complete control over data processing and storage. This <a href="https://github.com/pablobernabeu/secure_local_HPC_speech_transcription">production-grade transcription system</a> offers several compelling advantages over cloud alternatives:</p>
<div id="complete-data-sovereignty-and-gdpr-compliance" class="section level3">
<h3>Complete Data Sovereignty and GDPR Compliance</h3>
<p>By executing all processing on local or institutional infrastructure, the workflow ensures that sensitive audio content never leaves the controlled environment. This approach provides full GDPR compliance and satisfies the stringent data protection requirements common in academic research, healthcare and corporate environments. The system downloads pre-trained models once and runs them entirely offline, eliminating ongoing data transmission concerns.</p>
</div>
<div id="unlimited-scale-and-batch-processing-capabilities" class="section level3">
<h3>Unlimited Scale and Batch Processing Capabilities</h3>
<p>Unlike cloud services with arbitrary file size limitations, the local workflow can process audio files of any length and handle large-scale batch operations. The system supports parallel processing across multiple graphics processing unit (GPU) nodes in high-performance computing (HPC) environments, enabling researchers to transcribe hundreds of hours of audio content efficiently. The intelligent job scheduling system automatically detects available files and optimises resource allocation across computing clusters.</p>
</div>
<div id="reproducible-and-auditable-processing" class="section level3">
<h3>Reproducible and Auditable Processing</h3>
<p>Every aspect of the transcription pipeline is configurable and documented, from model selection and audio enhancement parameters to text processing rules and privacy protection settings. This transparency enables researchers to maintain detailed methodological records, reproduce results across different time periods and adjust processing parameters to optimise for specific audio conditions or research requirements.</p>
</div>
<div id="advanced-quality-control-and-post-processing" class="section level3">
<h3>Advanced Quality Control and Post-Processing</h3>
<p>The workflow incorporates sophisticated quality improvement algorithms that address common artefacts introduced by generative artificial intelligence models. These include automatic detection and removal of spurious repetitions, intelligent punctuation correction and context-aware personal name masking that prevents false positives whilst maintaining conversation flow and readability.</p>
</div>
<div id="flexible-audio-enhancement-pipeline" class="section level3">
<h3>Flexible Audio Enhancement Pipeline</h3>
<p>The system includes an optional audio enhancement stage that applies spectral noise reduction, dynamic range compression and signal amplification to improve transcription quality for challenging audio conditions. This preprocessing stage uses the first 0.5 seconds of each recording as a noise reference, enabling adaptive enhancement that adjusts to different recording environments.</p>
</div>
<div id="multi-model-support-and-future-proofing" class="section level3">
<h3>Multi-Model Support and Future-Proofing</h3>
<p>While optimised for OpenAI’s Whisper models, the architecture supports any <a href="https://huggingface.co/">HuggingFace</a>-compatible <a href="https://huggingface.co/models?pipeline_tag=automatic-speech-recognition">ASR model</a>, enabling researchers to experiment with specialised models for domain-specific applications or incorporate newer model releases as they become available. The modular design ensures long-term sustainability and adaptability to evolving transcription technologies.</p>
</div>
</div>
<div id="example-output" class="section level2">
<h2>Example Output</h2>
<p>The repository includes <a href="https://github.com/pablobernabeu/secure_local_HPC_speech_transcription/tree/main/example_output">example output files</a> demonstrating the system’s transcription quality and formatting. Below is an <a href="https://github.com/pablobernabeu/secure_local_HPC_speech_transcription/blob/main/example_output/transcripts/The%20modular%20mini-grammar_%20Building%20testable%20and%20reproducible%20artificial%20languages%20using%20FAIR%20principles_transcript.txt">example transcript</a>:</p>
<textarea readonly style='border-color: lightgrey; overflow: hidden; color: darkblue; font-size: 90%; min-width: 100%; height: 80vh; white-space: pre-wrap; overflow-wrap: normal; padding-right: 0.5em; padding-left: 1em;'></textarea>
<script>
fetch('https://raw.githubusercontent.com/pablobernabeu/secure_local_HPC_speech_transcription/refs/heads/main/example_output/transcripts/The%20modular%20mini-grammar_%20Building%20testable%20and%20reproducible%20artificial%20languages%20using%20FAIR%20principles_transcript.txt')
  .then(response => response.text())
  .then(data => {
    document.querySelector('textarea').textContent = data;
  });
</script>
</div>
<div id="technical-architecture" class="section level2">
<h2>Technical Architecture</h2>
<p>The system is implemented as a monolithic <a href="https://www.python.org/">Python</a> script, <a href="https://github.com/pablobernabeu/secure_local_HPC_speech_transcription/blob/main/transcription.py"><code>transcription.py</code></a>, which integrates all core components into a single, self-contained processing engine. This architectural choice prioritises simplicity, maintainability and ease of deployment without external module dependencies. The script orchestrates a multi-stage pipeline and resolves several critical technical challenges through a unified command-line interface.</p>
<script src='https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fsecure_local_HPC_speech_transcription%2Fblob%2Fmain%2Ftranscription.py%23L1096-L1117&style=a11y-dark&type=code&showFullPath=on&showCopy=on&showLineNumbers=on&showFileMeta=on'></script>
<div id="core-technical-solutions" class="section level3">
<h3>Core Technical Solutions</h3>
<p><strong>Challenge 1: GenAI-Generated Repetitions</strong>
A significant hurdle with generative AI models like Whisper is their tendency to produce spurious repetitions—a form of hallucination where the model gets “stuck” and repeats phrases.</p>
<ul>
<li><strong>Solution</strong>: The <code>--fix-spurious-repetitions</code> flag activates a sophisticated repetition detection algorithm. It analyses text patterns to distinguish between intentional, natural emphasis and AI-generated artefacts by considering the frequency, context, and structure of repeated segments.</li>
</ul>
<p><strong>Challenge 2: GenAI-Generated Language Switching</strong>
Whisper models can erroneously switch languages when interpreting phonetically ambiguous sounds.</p>
<ul>
<li><strong>Solution</strong>: The <code>--language</code> argument constrains the model to the designated language, preventing unwanted language switches while maintaining transcription accuracy.</li>
</ul>
<p><strong>Challenge 3: Intelligent and Private Name Masking</strong>
A key challenge is reliably identifying personal names while avoiding the masking of common words or technical terms (false positives).</p>
<ul>
<li><strong>Solution</strong>: The system uses a sophisticated, multi-tiered approach. The default is a curated database of over 1,793 names across nine languages, refined to minimise false positives on common words. For broader coverage, optional databases from Facebook (over 1.7 million names) can be enabled, though this increases the risk of false positives. Users can also provide custom lists of names to exclude from masking, which is useful for preserving the names of public figures or research team members.</li>
</ul>
<p><strong>Challenge 4: Scalable High-Performance Computing (HPC) Integration</strong>
The workflow is designed for large-scale batch processing in HPC environments using a Simple Linux Utility for Resource Management (SLURM) scheduler.</p>
<ul>
<li><strong>Solution</strong>: A collection of submission scripts provides dynamic job array sizing, automatically matching the number of jobs to the number of input files. The system intelligently optimises resource use by prioritising GPU allocation with a graceful fallback to CPU, ensuring continuous operation. It also includes comprehensive error detection and recovery. Users can override default HPC resource allocations using the <code>--memory</code> and <code>--time-limit</code> arguments for exceptionally large or complex files.</li>
</ul>
<p><strong>Challenge 5: Reproducible and Stable Environments</strong>
Creating a consistent Python environment across different platforms can be difficult due to dependency conflicts.</p>
<ul>
<li><strong>Solution</strong>: The project includes platform-agnostic setup scripts that automatically detect and adapt to different HPC module systems. They manage complex dependencies, particularly for <a href="https://pytorch.org/">PyTorch</a> and <a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a>, and use version pinning to ensure stability and reproducibility.</li>
</ul>
<p><strong>Challenge 6: Speaker Attribution (Diarisation)</strong>
Identifying who is speaking in a multi-speaker recording is a common requirement.</p>
<ul>
<li><strong>Solution</strong>: The system integrates <a href="https://github.com/pyannote/pyannote-audio"><code>pyannote.audio</code></a> for speaker diarisation, which can be enabled with the <code>--speaker-attribution</code> flag. This feature requires a <a href="https://huggingface.co/settings/tokens">HuggingFace user access token</a> for the <a href="https://huggingface.co/pyannote/speaker-diarization-3.1"><code>pyannote/speaker-diarization-3.1</code></a> model.</li>
</ul>
</div>
</div>
<div id="describing-the-method-in-publications" class="section level2">
<h2>Describing the Method in Publications</h2>
<p>For researchers incorporating this workflow into their methodology, the following description provides a comprehensive yet concise summary suitable for academic publications:</p>
<blockquote>
<p>Audio recordings were transcribed using <a href="https://huggingface.co/openai/whisper-large-v3">OpenAI’s Whisper large-v3 model</a>, a state-of-the-art automatic speech recognition system (Batista, 2024). The transcription workflow maintained full GDPR compliance through a local implementation where the pre-trained model was downloaded from the OpenAI repository on the <a href="https://huggingface.co/">Hugging Face platform</a> and executed entirely on institutional high-performance computing infrastructure, ensuring no audio data or transcription content was transmitted to or processed by third-party services.</p>
<p>The pipeline incorporated several processing stages: optional audio enhancement for improved signal quality using spectral noise reduction and dynamic range compression, configurable language specification to prevent unwanted language switching artefacts, automatic detection and removal of spurious text repetitions generated by the AI model, comprehensive spelling corrections and text formatting, and privacy protection through intelligent personal name masking that replaced detected names with anonymised placeholders whilst avoiding false positives on common conversational words.</p>
<p>Quality control measures included automatic repetition pattern detection to remove AI-generated artefacts, punctuation spacing corrections and context-aware text processing to maintain natural conversation flow. The system generated both plain text transcripts and formatted Microsoft Word documents with comprehensive processing metadata and timestamps for reproducibility and audit purposes.</p>
<p><strong>Reference</strong></p>
<p>Batista, J. R. (2024). <em>Learn OpenAI Whisper: Transform your understanding of GenAI through robust and accurate speech processing solutions</em>. Packt Publishing Ltd.</p>
</blockquote>
<div id="licence" class="section level3">
<h3>Licence</h3>
<p>This workflow is made available under the <a href="https://github.com/pablobernabeu/secure_local_HPC_speech_transcription/blob/main/Licence.md">Creative Commons 4.0 Attribution 4.0 International licence</a>.</p>
</div>
<div id="citation" class="section level3">
<h3>Citation</h3>
<div style="text-align: left;">
<p><a href="https://doi.org/10.5281/zenodo.17624830"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.17624830.svg" alt="DOI" style="display: inline-block; margin: 0;"></a></p>
</div>
<p>If you use this workflow in your research, please cite:</p>
<pre class="text"><code>@software{secure_local_HPC_speech_transcription,
  title={Secure and scalable speech transcription for local and HPC},
  author={Pablo Bernabeu},
  year={2025},
  doi={10.5281/zenodo.17624830},
  url={https://doi.org/10.5281/zenodo.17624830}
}</code></pre>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>This comprehensive transcription workflow represents a mature, production-ready alternative to cloud-based transcription services, specifically designed to meet the demanding requirements of research and development environments. By combining state-of-the-art AI models with robust engineering practices within a monolithic, easily maintainable architecture, the system delivers high-quality transcription capabilities whilst maintaining complete control over data processing, privacy protection and quality assurance.</p>
<p>The workflow’s emphasis on reproducibility, scalability and simplicity makes it particularly valuable for research applications where methodological rigour and long-term sustainability are essential. As speech recognition technology continues to evolve, this straightforward architecture ensures that researchers can incorporate new developments whilst maintaining the stability and clarity required for research workflows.</p>
<p>For organisations seeking to leverage advanced transcription capabilities without compromising data sovereignty or processing control, this workflow provides a compelling foundation for building sophisticated speech processing pipelines that can grow and adapt with emerging technologies and evolving research requirements.</p>
</div>
